#!/usr/bin/env python3
"""
Test the workflow components used by Streamlit app
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.workflow import MultiAgentWorkflow
from core import make_all_agent_llms
import json

def test_streamlit_workflow():
    """Test the workflow components used by Streamlit app."""
    
    print("ğŸ§ª Testing Streamlit Workflow Components")
    print("=" * 60)
    
    # Test LLM creation (same as in streamlit app)
    print("\nğŸ“‹ Test 1: LLM Creation")
    try:
        agent_llms = make_all_agent_llms("anthropic")
        print(f"   âœ… LLMs created successfully")
        print(f"   ğŸ”‘ Available LLMs: {list(agent_llms.keys())}")
    except Exception as e:
        print(f"   âŒ LLM creation failed: {e}")
        return False
    
    # Test workflow creation (same as in streamlit app)
    print("\nğŸ“‹ Test 2: Workflow Creation")
    try:
        workflow = MultiAgentWorkflow(
            agent_llms["perception"],
            agent_llms["research"], 
            agent_llms["analysis"],
            agent_llms["decision"],
            agent_llms["orchestrator"]
        )
        print(f"   âœ… Workflow created successfully: {type(workflow)}")
    except Exception as e:
        print(f"   âŒ Workflow creation failed: {e}")
        return False
    
    # Test cases that match Streamlit app usage
    test_cases = [
        {
            "name": "General skill analysis",
            "question": "What are our Python skill gaps?",
            "project_id": None,
            "scope": "company"
        },
        {
            "name": "Project-specific analysis (like in Streamlit)",
            "question": "Analyze the skill gaps for this specific project and provide structured recommendations.\n\nProject ID: proj_005\nProject Name: Cloud Migration Initiative\nRequired Skills: AWS, Terraform, Kubernetes, Linux, Python\nTimeline: 2024-07-01 to 2025-01-31\nBudget: $900,000\nScope: department",
            "project_id": "proj_005",
            "scope": "department"
        },
        {
            "name": "Empty question handling",
            "question": "",
            "project_id": None,
            "scope": "company"
        }
    ]
    
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“‹ Test {i+2}: {test_case['name']}")
        print(f"   Question: {test_case['question'][:100]}{'...' if len(test_case['question']) > 100 else ''}")
        print(f"   Project ID: {test_case['project_id']}")
        print(f"   Scope: {test_case['scope']}")
        
        try:
            # Test the workflow.run method (same as in streamlit app)
            result = workflow.run(
                test_case['question'],
                project_id=test_case['project_id'],
                scope=test_case['scope']
            )
            
            if result is None:
                print(f"   âš ï¸ Analysis returned None (expected for empty question)")
                results.append(True)
                continue
                
            print(f"   ğŸ“Š Analysis completed successfully")
            print(f"   ğŸ”‘ Result keys: {list(result.keys())}")
            
            # Check for required fields that Streamlit app expects
            required_fields = ['intent', 'analysis', 'decision']
            missing_fields = [field for field in required_fields if field not in result]
            
            if missing_fields:
                print(f"   âš ï¸ Missing fields: {missing_fields}")
                results.append(False)
            else:
                print(f"   âœ… All required fields present")
                
                # Check analysis quality
                analysis = result.get('analysis', '')
                if analysis and 'Analysis pending' not in analysis and len(analysis) > 100:
                    print(f"   âœ… Analysis contains real data")
                else:
                    print(f"   âš ï¸ Analysis contains placeholder data")
                
                # Check decision quality
                decision = result.get('decision', '')
                if decision and 'Decision pending' not in decision and len(decision) > 100:
                    print(f"   âœ… Decision contains real data")
                else:
                    print(f"   âš ï¸ Decision contains placeholder data")
                
                # Try to parse analysis and decision as JSON (Streamlit app does this)
                try:
                    analysis_json = json.loads(analysis) if analysis else {}
                    decision_json = json.loads(decision) if decision else {}
                    
                    print(f"   âœ… Analysis JSON valid: {list(analysis_json.keys())}")
                    print(f"   âœ… Decision JSON valid: {list(decision_json.keys())}")
                    
                    # Test specific fields that Streamlit app displays
                    if 'skill_gaps' in analysis_json:
                        print(f"   ğŸ“Š Skill gaps found: {len(analysis_json['skill_gaps'])}")
                    if 'upskilling' in analysis_json:
                        print(f"   ğŸ“Š Upskilling recommendations: {len(analysis_json['upskilling'])}")
                    if 'selected_strategy' in decision_json:
                        print(f"   ğŸ“Š Selected strategy: {decision_json['selected_strategy'].get('strategy_name', 'N/A')}")
                    
                    results.append(True)
                    
                except json.JSONDecodeError as e:
                    print(f"   âŒ JSON parsing error: {e}")
                    results.append(False)
                
        except Exception as e:
            print(f"   âŒ Analysis error: {e}")
            import traceback
            traceback.print_exc()
            results.append(False)
    
    # Test error handling scenarios that could occur in Streamlit
    print(f"\nğŸ”§ Testing Streamlit Error Handling Scenarios")
    
    # Test with invalid project ID
    try:
        result = workflow.run(
            "Test question",
            project_id="invalid_project_id",
            scope="company"
        )
        print(f"   âœ… Invalid project ID handled gracefully")
        results.append(True)
    except Exception as e:
        print(f"   âŒ Invalid project ID handling failed: {e}")
        results.append(False)
    
    # Test with very long question
    try:
        long_question = "What are our skill gaps? " * 100
        result = workflow.run(long_question, project_id=None, scope="company")
        print(f"   âœ… Long question handled gracefully")
        results.append(True)
    except Exception as e:
        print(f"   âŒ Long question handling failed: {e}")
        results.append(False)
    
    # Test retry mechanism (like in Streamlit app)
    print(f"\nğŸ”§ Testing Retry Mechanism")
    try:
        # Simulate the retry logic from Streamlit app
        try:
            result = workflow.run("Complex question with many details", project_id="proj_005", scope="department")
        except Exception as workflow_error:
            print(f"   âš ï¸ First attempt failed: {workflow_error}")
            # Try again with a simpler question (like in Streamlit)
            simple_question = "Analyze skill gaps for project Cloud Migration Initiative requiring skills: AWS, Terraform, Kubernetes"
            result = workflow.run(simple_question, project_id="proj_005", scope="department")
            print(f"   âœ… Retry with simpler question succeeded")
        
        results.append(True)
    except Exception as e:
        print(f"   âŒ Retry mechanism failed: {e}")
        results.append(False)
    
    # Summary
    passed = sum(results)
    total = len(results)
    print(f"\nğŸ“Š Streamlit Workflow Test Results:")
    print(f"   âœ… Passed: {passed}/{total}")
    print(f"   âŒ Failed: {total - passed}/{total}")
    print(f"   ğŸ“ˆ Success Rate: {passed/total*100:.1f}%")
    
    return passed == total

if __name__ == "__main__":
    test_streamlit_workflow()
